{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["p1gpbaYGHLB4","0A2hpwIu9JIJ","2f142dde","e889f975","kLNWW-qNZi_1","dtJBXK17Z830","SZkjeThubiCQ","BObcW0GIYeIE","vhxn0TjAf64T","c52gPasdfhXY","9yAq8xvYz8G0","TPneLWzz5VDW","wNGMYsAA_c_x","xO3m8rAg_glA","8ZrfLjVHfqjk","gWFMq6kBL52F"],"toc_visible":true,"authorship_tag":"ABX9TyM3vP3DgGmenZKQaixANGYQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<br>\n","\n","# <center> Sudoku Solving using Computer Vision"],"metadata":{"id":"9iLmRIXmREkI"}},{"cell_type":"markdown","source":["<br>\n","\n","## <center> Part 01 : Digits Classification"],"metadata":{"id":"29daA_8jT3zy"}},{"cell_type":"markdown","source":["<br>\n","\n","---\n","\n","<br>\n"],"metadata":{"id":"C51Ho_qvREUX"}},{"cell_type":"markdown","source":["<br>\n","\n","## Colab Configuration"],"metadata":{"id":"p1gpbaYGHLB4"}},{"cell_type":"markdown","source":["### Mount Google Drive"],"metadata":{"id":"0A2hpwIu9JIJ"}},{"cell_type":"code","source":["'''\n","    This is required if the code runs in Google Colab.\n","    - this code will mount Google Drive for Colab.\n","    - the code needs to run only once.\n","'''\n","\n","# # uncomment the below code, run and then comment again.\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"O4_sQAVw83Cx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676324007882,"user_tz":0,"elapsed":2227,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}},"outputId":"865a7b4d-a84a-4d75-e3e5-673508353a53"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["<br>\n","\n","### Defining Root Directory"],"metadata":{"id":"L-ETilP4FEhn"}},{"cell_type":"code","source":["# this directory will be used as Root Directory to read/write any file\n","rootDir = '/content/drive/MyDrive/_ML/Sudoku/'"],"metadata":{"id":"Dq3G16RuFMbG","executionInfo":{"status":"ok","timestamp":1676324007883,"user_tz":0,"elapsed":7,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"W-GouWbJQowr","executionInfo":{"status":"ok","timestamp":1676324007884,"user_tz":0,"elapsed":7,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2f142dde"},"source":["<br>\n","\n","## Import Libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"4677354a","executionInfo":{"status":"ok","timestamp":1676324008839,"user_tz":0,"elapsed":961,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"outputs":[],"source":["# importing all the required libraries\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import os, random\n","import cv2"]},{"cell_type":"code","source":["import json"],"metadata":{"id":"xNcXkcwtFKzm","executionInfo":{"status":"ok","timestamp":1676324008841,"user_tz":0,"elapsed":7,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e889f975"},"source":["<br>\n","\n","## Reading DataSet"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"m1mzSdRQ_YMe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676324058737,"user_tz":0,"elapsed":49902,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}},"outputId":"0bd8c9e7-8efb-4a47-8028-2c20f49dbc53"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------------------\n"," Reading Digit Dataset . . .\n","------------------------------------------\n","   > Loading Class : 0\n","   > Loading Class : 1\n","   > Loading Class : 2\n","   > Loading Class : 3\n","   > Loading Class : 4\n","   > Loading Class : 5\n","   > Loading Class : 6\n","   > Loading Class : 7\n","   > Loading Class : 8\n","   > Loading Class : 9\n","------------------------------------------\n","   Digits loading successfull.\n","  Total datapoints : 10160\n","------------------------------------------\n"]}],"source":["digitsPath = rootDir + '02. DataSet/Digits/Images'\n","\n","data = os.listdir(digitsPath)\n","\n","data_X = []\n","data_y = []\n","\n","digit_class_name = []\n","\n","data_classes = len(data)\n","\n","print('------------------------------------------')\n","print(' Reading Digit Dataset . . .')\n","print('------------------------------------------')\n","\n","for class_name in range(data_classes):\n","  digit_class_name.append( str(class_name) )\n","\n","  print(f'   > Loading Class : {class_name}')\n","  \n","  class_path = digitsPath + '/' + str(class_name)\n","  image_list = os.listdir(class_path)\n","\n","  for image_name in image_list:\n","    pic = cv2.imread(class_path + '/' + image_name)\n","    pic = cv2.resize(pic, (32,32))\n","    data_X.append(pic)\n","    data_y.append(class_name)\n","\n","\n","if len(data_X) == len(data_y):\n","  print('------------------------------------------')\n","  print('   Digits loading successfull.')\n","  print(f'  Total datapoints : {len(data_X)}')\n","  print('------------------------------------------')\n","\n","data_X = np.array(data_X)\n","data_y = np.array(data_y)"]},{"cell_type":"markdown","source":["<br>\n","\n","## Spliting DataSet"],"metadata":{"id":"kLNWW-qNZi_1"}},{"cell_type":"code","source":["# importing library\n","from sklearn.model_selection import train_test_split\n","\n","# spliting the data into train and test data\n","# 0.05% data will be used for test and 99.95% for train\n","train_X, test_X, train_y, test_y = train_test_split(\n","                                        data_X,\n","                                        data_y,\n","                                        test_size = 0.05\n","                                    )\n","\n","# spliting further the train data into train and validation data\n","# 20% data will be used for validation and 80% for train\n","train_X, valid_X, train_y, valid_y = train_test_split(\n","                                        train_X,\n","                                        train_y,\n","                                        test_size = 0.2\n","                                    )\n","\n","\n","# displaying the shape of the splitted data\n","print(f\"  Training Set Shape = {train_X.shape}\")\n","print(f\"Validation Set Shape = {valid_X.shape}\")\n","print(f\"      Test Set Shape = {test_X.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WGwSZKanUsy1","executionInfo":{"status":"ok","timestamp":1676324058738,"user_tz":0,"elapsed":26,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}},"outputId":"b5b89d92-c85c-4cb6-841b-e4ed570e353a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["  Training Set Shape = (7721, 32, 32, 3)\n","Validation Set Shape = (1931, 32, 32, 3)\n","      Test Set Shape = (508, 32, 32, 3)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"H7dLzuyaZ1o6","executionInfo":{"status":"ok","timestamp":1676324058739,"user_tz":0,"elapsed":22,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","## Image Preprocessing"],"metadata":{"id":"dtJBXK17Z830"}},{"cell_type":"code","source":["# importing the image processing library\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","# Preprocessing the images for NeuralNet\n","def preprocess(img):\n","    '''\n","        This function converts an image to gray scale,\n","        equalizes the histogram, and normalizes the image.\n","\n","        Parameter\n","        ---------\n","        img\n","            An image file\n","\n","        Return\n","        ------\n","        ret\n","            Processed Image\n","    '''\n","\n","    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) #making image grayscale\n","    img = cv2.equalizeHist(img) #Histogram equalization to enhance contrast\n","    img = img/255 #normalizing\n","    return img\n","\n","# preprocessing the image data\n","train_X = np.array( list(map(preprocess, train_X)) )\n","test_X  = np.array( list(map(preprocess,  test_X)) )\n","valid_X = np.array( list(map(preprocess, valid_X)) )\n","\n","# reshaping the images\n","train_X = train_X.reshape(train_X.shape[0], train_X.shape[1], train_X.shape[2], 1)\n","test_X  = test_X.reshape( test_X.shape[0],  test_X.shape[1],  test_X.shape[2],  1)\n","valid_X = valid_X.reshape(valid_X.shape[0], valid_X.shape[1], valid_X.shape[2], 1)\n","\n","# augmentation\n","datagen = ImageDataGenerator(\n","                width_shift_range=0.1, \n","                height_shift_range=0.1, \n","                zoom_range=0.2, \n","                shear_range=0.1, \n","                rotation_range=10\n","            )\n","datagen.fit(train_X)"],"metadata":{"id":"nWh8LpzkZ4fc","executionInfo":{"status":"ok","timestamp":1676324061654,"user_tz":0,"elapsed":2936,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from keras.utils.np_utils import to_categorical\n","\n","# One hot encoding of the labels\n","train_y = to_categorical(train_y, data_classes)\n","test_y = to_categorical(test_y, data_classes)\n","valid_y = to_categorical(valid_y, data_classes)"],"metadata":{"id":"Vc4I3LXsbRG7","executionInfo":{"status":"ok","timestamp":1676324061659,"user_tz":0,"elapsed":15,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","## a. Model Design"],"metadata":{"id":"SZkjeThubiCQ"}},{"cell_type":"code","source":["# importing the required libraries\n","from keras.models import Sequential\n","from keras.layers import Activation, Dropout, Dense, Flatten, BatchNormalization, Conv2D, MaxPooling2D\n","from keras.optimizers import RMSprop"],"metadata":{"id":"KvsFKpRFbuCW","executionInfo":{"status":"ok","timestamp":1676324061659,"user_tz":0,"elapsed":13,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Creating a Convolutional Neural Network (CNN)\n","\n","model = Sequential()\n","\n","model.add((Conv2D(60,(5,5),input_shape=(32, 32, 1) ,padding = 'Same' ,activation='relu')))\n","model.add((Conv2D(60, (5,5),padding=\"same\",activation='relu')))\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","#model.add(Dropout(0.25))\n","\n","model.add((Conv2D(30, (3,3),padding=\"same\", activation='relu')))\n","model.add((Conv2D(30, (3,3), padding=\"same\", activation='relu')))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n","model.add(Dropout(0.5))\n","\n","model.add(Flatten())\n","model.add(Dense(500,activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(10, activation='softmax'))\n"],"metadata":{"id":"lsmj9HDkbk8L","executionInfo":{"status":"ok","timestamp":1676324062005,"user_tz":0,"elapsed":358,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# displaying the summary of the model\n","model.summary()"],"metadata":{"id":"k5nvPqX_kX_S","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676324062006,"user_tz":0,"elapsed":20,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}},"outputId":"43bd8b93-ef78-47ed-9a38-4e1363b5fb7b"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 32, 32, 60)        1560      \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 32, 32, 60)        90060     \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 16, 16, 60)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 16, 16, 30)        16230     \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 16, 16, 30)        8130      \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 8, 8, 30)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout (Dropout)           (None, 8, 8, 30)          0         \n","                                                                 \n"," flatten (Flatten)           (None, 1920)              0         \n","                                                                 \n"," dense (Dense)               (None, 500)               960500    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 500)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 10)                5010      \n","                                                                 \n","=================================================================\n","Total params: 1,081,490\n","Trainable params: 1,081,490\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# defining the optimizer\n","optimizer = RMSprop(\n","                learning_rate=0.001, \n","                rho=0.9, \n","                epsilon = 1e-08, \n","                decay=0.0\n","              )\n","\n","#Compiling the model\n","model.compile(\n","    optimizer = optimizer,\n","    loss='categorical_crossentropy',\n","    metrics=['accuracy']\n","  )"],"metadata":{"id":"rtJtcpoeYNV8","executionInfo":{"status":"ok","timestamp":1676324062006,"user_tz":0,"elapsed":5,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","## b. Model Train"],"metadata":{"id":"BObcW0GIYeIE"}},{"cell_type":"code","source":["# imprting early stopping callbacks from keras\n","from keras.callbacks import EarlyStopping\n","\n","# defining early stopping\n","early_stop = EarlyStopping( \n","    monitor = 'val_loss', \n","    mode = 'min', \n","    min_delta = 0.001, # minimium amount of change to count as an improvement\n","    verbose = 1, \n","    patience = 20,\n","    restore_best_weights = True, \n","  )"],"metadata":{"id":"nWpU33g6IlQp","executionInfo":{"status":"ok","timestamp":1676324062006,"user_tz":0,"elapsed":5,"user":{"displayName":"SHOHRAB HOSSAIN 1408003","userId":"01228065728446237063"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["#Fit the model\n","\n","history = model.fit( \n","              datagen.flow(\n","                  train_X, \n","                  train_y, \n","                  batch_size=32\n","              ),\n","              epochs = 30, \n","              validation_data = (valid_X, valid_y),\n","              verbose = 2,\n","              steps_per_epoch= 200,\n","              callbacks = [early_stop]\n","          )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xblkVukib21-","outputId":"1aeec933-723e-450e-cee3-0ad4b3c97f68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","200/200 - 153s - loss: 0.8709 - accuracy: 0.7013 - val_loss: 0.1016 - val_accuracy: 0.9658 - 153s/epoch - 767ms/step\n","Epoch 2/30\n","200/200 - 151s - loss: 0.2915 - accuracy: 0.9081 - val_loss: 0.0577 - val_accuracy: 0.9839 - 151s/epoch - 753ms/step\n","Epoch 3/30\n","200/200 - 166s - loss: 0.2000 - accuracy: 0.9395 - val_loss: 0.0522 - val_accuracy: 0.9839 - 166s/epoch - 829ms/step\n","Epoch 4/30\n","200/200 - 168s - loss: 0.1570 - accuracy: 0.9522 - val_loss: 0.0234 - val_accuracy: 0.9912 - 168s/epoch - 839ms/step\n","Epoch 5/30\n","200/200 - 153s - loss: 0.1369 - accuracy: 0.9567 - val_loss: 0.0317 - val_accuracy: 0.9891 - 153s/epoch - 764ms/step\n","Epoch 6/30\n","200/200 - 151s - loss: 0.1204 - accuracy: 0.9644 - val_loss: 0.0274 - val_accuracy: 0.9922 - 151s/epoch - 755ms/step\n","Epoch 7/30\n"]}]},{"cell_type":"code","source":["# Testing the model on the test set\n","\n","score = model.evaluate(test_X, test_y, verbose=0)\n","print('Test Score = ',score[0])\n","print('Test Accuracy =', score[1])"],"metadata":{"id":"21xuiOOsb2y6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","## c. Model Evaluaiton"],"metadata":{"id":"vhxn0TjAf64T"}},{"cell_type":"markdown","source":["<br>\n","\n","### a. Visualizing Loss Analysis"],"metadata":{"id":"c52gPasdfhXY"}},{"cell_type":"code","source":["# accessing history\n","arch = history.history\n","# model_loss = pd.DataFrame( model.history.history )\n","model_loss = pd.DataFrame( history.history )\n","\n","# plotting the history\n","sns.set_theme(style=\"darkgrid\")\n","model_loss[['loss', 'val_loss']].plot(figsize=(12,8))"],"metadata":{"id":"f7NPGN1Te8RJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","### b. Classification Report and Confusion Matrix"],"metadata":{"id":"qxyc5IawfX6a"}},{"cell_type":"markdown","source":["<br>\n","\n","#### Preparing Data"],"metadata":{"id":"9yAq8xvYz8G0"}},{"cell_type":"code","source":["# copying the original X and y test data\n","X_test_new = test_X\n","y_test_new = test_y"],"metadata":{"id":"kJT5-TYCzLzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fill_missing_class() :\n","  '''\n","    This function will fill any Intent-Class missing in 'y_test'.\n","    The sklearn Train_Test_Split generates Train and Test data randomly. So any classes can be missed in the test data.\n","  '''\n","\n","  # accessing the global numpy array to write\n","  global X_test_new\n","  global y_test_new\n","\n","  # finding the classes in train and test data\n","  yTrainArgmax = set( train_y.argmax(axis=1) )\n","  yTestArgmax  = set( test_y.argmax(axis=1) )\n","\n","  # finding the intent classes missing in 'y_test' by comparing with 'y_train'\n","  missing_classes = yTrainArgmax.difference(yTestArgmax)\n","  \n","  # convering to List\n","  missing_classes = list(missing_classes)\n","\n","  # iterating the missing classes \n","  for mc in missing_classes:\n","\n","    # finding the missing classes that exist in 'y_train'\n","    indices_mc = np.where(train_y.argmax(axis=1) == mc)[0]  # indices of any missing class\n","\n","    # if any missing class has more then 5 entity then take first 5 otherwise take whatever entity the class has\n","    if len(indices_mc) > 4 :\n","      indices_mc = indices_mc[:5]\n","\n","    # iteraring the indices, and appending intents from 'X_train' and 'y_train' using the indices\n","    for i in indices_mc:\n","      X_test_new = np.append(X_test_new, [train_X[i]], axis=0)\n","      y_test_new = np.append(y_test_new, [train_y[i]], axis=0)\n"],"metadata":{"id":"QHEh3Txy3M42"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# filling any intents missing in 'y_test'\n","fill_missing_class()"],"metadata":{"id":"k54uWwLx7OgU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","#### Predicting on 'y_test' data"],"metadata":{"id":"TPneLWzz5VDW"}},{"cell_type":"code","source":["# prediction on test set\n","predictions = model.predict(X_test_new)"],"metadata":{"id":"HNSijPAffIeT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# finding the actual classes\n","y_test_argmax = y_test_new.argmax(axis=1)\n","\n","# finding the predicted classes\n","predictions_argmax = predictions.argmax(axis=1)"],"metadata":{"id":"vvHf5_W9Bhzu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","#### Classification Report"],"metadata":{"id":"wNGMYsAA_c_x"}},{"cell_type":"code","source":["# importing the library\n","from sklearn.metrics import classification_report\n","\n","# classification report\n","_CR = classification_report(y_test_argmax, predictions_argmax, target_names=digit_class_name)\n","\n","# printing the report\n","print( _CR )"],"metadata":{"id":"m_fNeyJm_DbV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","#### Confusion Matrix"],"metadata":{"id":"xO3m8rAg_glA"}},{"cell_type":"code","source":["# importing the library\n","from sklearn.metrics import confusion_matrix\n","\n","# calculationg confusion matrix\n","confusionMat = confusion_matrix(y_test_argmax, predictions_argmax)"],"metadata":{"id":"FSTyjXxUfIby"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# defining figure size\n","plt.figure(figsize=(12,15))\n","\n","# plotting heatmap of confusion matrix\n","sns.heatmap(confusionMat, square=True, fmt='d', \n","            cbar=False, cmap='YlGnBu',  \n","            xticklabels=digit_class_name,\n","            yticklabels=digit_class_name,\n","            annot=True,\n","            annot_kws={\n","                \"fontsize\":12,\n","                'fontweight': 'bold',\n","                },\n","          )\n","\n","# defining the label names for X-axis and Y-axis\n","plt.xlabel('Predicted Class', fontsize=14)\n","plt.ylabel('Actual Class', fontsize=14)"],"metadata":{"id":"9HB9h9C9fIZr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","\n","### c. Prediction on New Data"],"metadata":{"id":"8ZrfLjVHfqjk"}},{"cell_type":"markdown","source":["Creating Data"],"metadata":{"id":"7mCYgPhOfsrp"}},{"cell_type":"code","source":["# accessing the main data set\n","_data = data_X\n","_target = data_y\n","\n","# defining random position\n","pos = random.randint(0, len(_data))\n","\n","# collecting data from main dataset\n","new_data = _data[pos]\n","\n","# preprocessing the new data and converting the target to categorical\n","new_data = preprocess(new_data)\n","_target = to_categorical(_target, data_classes)\n","\n","\n","# reshaping the data\n","new_data = new_data.reshape(-1, 32, 32, 1)\n","\n","# extracting true result\n","true_result_list = _target[pos]\n","true_class  = int( np.argmax(true_result_list) )"],"metadata":{"id":"mGqpUGKXfIXf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Predicting Class for Created Data"],"metadata":{"id":"IkqHVEhDfwto"}},{"cell_type":"code","source":["# predicting result for new_data\n","predicted_class = model.predict(new_data)\n","\n","# extracting the maximum predicted calss\n","predicted_class = int( np.argmax(predicted_class, axis=1) )"],"metadata":{"id":"IxTlh4O5fIVG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Extracting Class name"],"metadata":{"id":"wsSFlwqZf0Ke"}},{"cell_type":"code","source":["# extracting predicted class name\n","predicted_class_name = digit_class_name[predicted_class]\n","print(f\"\\n The predicted class is : '{predicted_class_name}'\")\n","\n","# extracting actual class name\n","true_class_name = digit_class_name[true_class]\n","print(f\"\\n The actual class is : '{true_class_name}'\")"],"metadata":{"id":"ONNE1N8wfISm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<br>\n","<br>\n","\n","## Save Model"],"metadata":{"id":"gWFMq6kBL52F"}},{"cell_type":"code","source":["# defining the name of the model\n","modelName = 'DigitClassifier.h5'\n","\n","# creating the path\n","path = rootDir + '03. Trainned Model/' + modelName\n","\n","# save model\n","model.save( path )"],"metadata":{"id":"v52jX35vL4_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AWXAWTqv_-lI"},"execution_count":null,"outputs":[]}]}